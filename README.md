# Knowledge_DIstillation_in_Deep_Learning
Knowledge Distillation is a model-compression technique where a large, high-performing model (Teacher) transfers its knowledge to a smaller, faster model (Student).
